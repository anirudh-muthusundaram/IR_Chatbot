{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Scraped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aniru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aniru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aniru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Health = pd.read_csv(\"./Data/Health.csv\")\n",
    "Environment = pd.read_csv(\"./Data/Environment.csv\")\n",
    "Technology = pd.read_csv(\"./Data/Technology.csv\")\n",
    "Economy = pd.read_csv(\"./Data/Economy.csv\")\n",
    "Entertainment = pd.read_csv(\"./Data/Entertainment.csv\")\n",
    "Sports = pd.read_csv(\"./Data/Sports.csv\")\n",
    "Politics = pd.read_csv(\"./Data/Politics.csv\")\n",
    "Education = pd.read_csv(\"./Data/Education.csv\")\n",
    "Travel = pd.read_csv(\"./Data/Travel.csv\")\n",
    "Food = pd.read_csv(\"./Data/Food.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize stemmer, lemmatizer and stop words\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(re.sub(r\"[^a-zA-Z]\", \" \", text.lower()))\n",
    "    \n",
    "    # Stemming, Lemmatization and Stopwords removal.\n",
    "    text_processed = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    #combine everything onto processed text\n",
    "    return ' '.join(text_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the location of the Data that needs to be preprocessed.\n",
    "Data_location = ['./Data/Health.csv', './Data/Environment.csv', './Data/Technology.csv', \n",
    "              './Data/Economy.csv', './Data/Entertainment.csv', './Data/Sports.csv', \n",
    "              './Data/Politics.csv', './Data/Education.csv', './Data/Travel.csv', './Data/Food.csv']\n",
    "\n",
    "# Creating an empty DataFrame to copy all the preprocessed text and other columns along with it.\n",
    "preprocessed_text = pd.DataFrame()\n",
    "\n",
    "# CSV files are loaded in each loop.\n",
    "# Every single CSV is a Topic.\n",
    "# Each and every topic's Summary is preprocessed and concat onto the preprocessed DataFrame.\n",
    "# Every other column are also appended onto the DataFrame.\n",
    "for iterate in Data_location:\n",
    "    dataFrame = pd.read_csv(iterate)\n",
    "    dataFrame['Summary'] = dataFrame['Summary'].apply(preprocess)\n",
    "    preprocessed_text = pd.concat([preprocessed_text, dataFrame], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        56000 non-null  object\n",
      " 1   Title        56000 non-null  object\n",
      " 2   Summary      56000 non-null  object\n",
      " 3   URL          56000 non-null  object\n",
      " 4   Revision ID  56000 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Required DataFrame info\n",
    "print(preprocessed_text.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the DataFrame onto a csv file for indexing.\n",
    "csv_file_name = \"preprocessed_text.csv\"\n",
    "preprocessed_text.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        56000 non-null  object\n",
      " 1   Title        56000 non-null  object\n",
      " 2   Summary      56000 non-null  object\n",
      " 3   URL          56000 non-null  object\n",
      " 4   Revision ID  56000 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of Every Grading Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Topic        56000 non-null  object\n",
      " 1   Title        56000 non-null  object\n",
      " 2   Summary      56000 non-null  object\n",
      " 3   URL          56000 non-null  object\n",
      " 4   Revision ID  56000 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Checking the characters length of Summary in each document with respect to the grading criteria (Summary length).\n",
    "# Total Docs = 5500\n",
    "# Docs in Each Topic = 550\n",
    "# All Fields are named as required from topic, title, summary, url and revision_id.\n",
    "data = pd.read_csv('preprocessed_text.csv') \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary\n",
       "True     52703\n",
       "False     3297\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = data['Summary'].apply(lambda x: len(x) >= 200)\n",
    "char_count = summary.value_counts()\n",
    "char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.887499999999999 %\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(data)\n",
    "below_required_chars = (char_count[False] / total_docs) * 100\n",
    "\n",
    "# Checking whether Not more than 5% of documents of length less than 200 characters in the summary.\n",
    "print(below_required_chars,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether Not more than 5% of documents should have any other characters other than alphanumeric.\n",
    "alphabet_check = data['Summary'].apply(lambda x: any(not c.isalnum() and not c.isspace() for c in x))\n",
    "total_count = alphabet_check.sum()\n",
    "percentage_non_alpha = (total_count / len(data)) * 100\n",
    "\n",
    "# Checking whether the percentage is below 5%.\n",
    "# returns True, if it's below.\n",
    "# returns False if it's above.\n",
    "result = percentage_non_alpha <= 5\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
